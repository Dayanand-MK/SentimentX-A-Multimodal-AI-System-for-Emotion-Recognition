# ğŸ­ SentimentX: A Multimodal AI System for Emotion Recognition

**SentimentX** is a cutting-edge AI system that understands human emotions through **video**, **audio**, and **text**. It uses powerful deep learning models like CNNs, Transformers, and BERT, and wraps everything in a sleek and user-friendly **Tkinter GUI**.

![SentimentX Banner](https://via.placeholder.com/800x200.png?text=SentimentX+-+Emotion+Recognition+System)

---

## ğŸ’¡ Key Features

- ğŸ¥ **Video Emotion Detection**  
  Real-time facial emotion recognition using CNN + Spatial Transformer Network.

- ğŸ™ï¸ **Audio Sentiment Analysis**  
  Emotion prediction from voice using a hybrid Transformer-CNN model.

- ğŸ“ **Text Sentiment Analysis**  
  Sentiment classification using BERT/DistilBERT fine-tuned models.

- ğŸ§  **Unified GUI**  
  A full-screen Tkinter interface to interact with all three modalities.

- ğŸ”Š Voice response with `pyttsx3`  
  Emotion results are announced via TTS for video predictions.

---

## ğŸ—‚ï¸ Project Structure


SentimentX/
â”‚
â”œâ”€â”€ Main.py                     # ğŸ§  GUI entry point
â”œâ”€â”€ Execution.txt               # âœ… Ordered process flow
â”‚
â”œâ”€â”€ video_analysis/
â”‚   â”œâ”€â”€ sentivideo.py           # Video emotion detection logic
â”‚   â”œâ”€â”€ deep_emotion.py         # CNN + STN model
â”‚   â”œâ”€â”€ train_emotion_model.py  # Model training script
â”‚   â”œâ”€â”€ data_load.py            # Custom PyTorch dataset
â”‚   â”œâ”€â”€ prepare_dataset.py      # Dataset preprocessing (CSV)
â”‚   â””â”€â”€ deep_emotion-100-128-0.005.pt
â”‚
â”œâ”€â”€ audio_analysis/
â”‚   â”œâ”€â”€ torchscript.py          # Converts model to TorchScript
â”‚   â”œâ”€â”€ emotion_model.pth       # Pretrained model for audio emotion
â”‚
â”œâ”€â”€ text_analysis/
â”‚   â”œâ”€â”€ sentitext.py            # Training + prediction using DistilBERT
â”‚   â”œâ”€â”€ sentitext_load.py       # Model loader (BERT version)
â”‚   â”œâ”€â”€ bert_sentiment_model/   # Saved BERT model and label_map.json
â”‚   â””â”€â”€ sentimentds.csv         # Sample dataset


---

## â–¶ï¸ Execution Flow

To set up and run this project, follow these steps **in order**:

1. `prepare_dataset.py`  
   â¤ Converts CK+ dataset folders into `Train1.csv` and `Test_1.csv`.

2. `train_emotion_model.py`  
   â¤ Trains and saves the CNN model (`deep_emotion-100-128-0.005.pt`).

3. `deep_emotion.py`  
   â¤ Contains model architecture used in training and inference.

4. `data_load.py`  
   â¤ Provides dataset class used by PyTorch's DataLoader.

5. `sentivideo.py`  
   â¤ Starts webcam-based real-time facial emotion recognition.

6. `Main.py`  
   â¤ Runs the GUI combining all three emotion detection modules.

---

## âš™ï¸ Installation

### 1. Clone this repo

git clone https://github.com/<your-username>/SentimentX.git
cd SentimentX


### 2. Install dependencies

pip install -r requirements.txt


You may also need:

pip install torch torchvision transformers scikit-learn opencv-python librosa pygame


### 3. Model Placement

| Model | Location |
|-------|----------|
| `deep_emotion-100-128-0.005.pt` | `video_analysis/` |
| `emotion_model.pth`             | `audio_analysis/` |
| `bert_sentiment_model/` folder  | `text_analysis/` |

---

## ğŸš€ Run the Application


python Main.py


---

## ğŸ§‘â€ğŸ’» Developers

ğŸ‘¨â€ğŸ’» **Dayanand M K** â€” *Lead Developer*  
ğŸ‘¨â€ğŸ’» Balaji S  
ğŸ‘¨â€ğŸ’» Praveen K S  

Batch: 9 â€” 2nd Year CSE (AI & ML)  
Vel Tech High Tech Dr. Rangarajan Dr. Sakunthala Engineering College

---

## ğŸ”® Future Scope

- Fuse results from all three modalities to make smarter emotional decisions  
- Add support for complex, subtle emotions  
- Build VR/AR integrations for immersive feedback

---

## ğŸ“„ License

MIT License Â© 2025 Dayanand M K and Team

---

## ğŸ“Œ Screenshot

![GUI Screenshot](https://via.placeholder.com/800x500.png?text=SentimentX+GUI)

---
